{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO7nE7jKL1Mb5wKsklGN8OX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alvumu/TFM/blob/main/Llama2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6O3_Rmvkntd",
        "outputId": "ae55e467-b9f9-473e-96a7-e87598dddd4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Llama-2-7b-hf'...\n",
            "remote: Enumerating objects: 33, done.\u001b[K\n",
            "remote: Total 33 (delta 0), reused 0 (delta 0), pack-reused 33\u001b[K\n",
            "Unpacking objects: 100% (33/33), 491.74 KiB | 5.23 MiB/s, done.\n",
            "Filtering content: 100% (7/7), 9.65 GiB | 3.16 MiB/s, done.\n",
            "Encountered 4 file(s) that may not have been copied correctly on Windows:\n",
            "\tpytorch_model-00003-of-00003.bin\n",
            "\tpytorch_model-00002-of-00003.bin\n",
            "\tpytorch_model-00001-of-00003.bin\n",
            "\tmodel-00001-of-00002.safetensors\n",
            "\n",
            "See: `git lfs help smudge` for more details.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://huggingface.co/NousResearch/Llama-2-7b-hf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer,AutoModelForCauslLM, pipeline\n",
        "import torch\n",
        "from peft import PeftModel\n",
        "import csv\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "946TlbLblPD2",
        "outputId": "983806ac-7b1f-49fd-abfc-7a9454e08351"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'AutoModelForCauslLM' from 'transformers' (/usr/local/lib/python3.10/dist-packages/transformers/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-8ff64c273766>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mAutoModelForCauslLM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'AutoModelForCauslLM' from 'transformers' (/usr/local/lib/python3.10/dist-packages/transformers/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"NousResearch/Llama-2-7b-hf\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "Qs4iMFMLlySH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = PeftModel.from_pretrained(model, \"LLMTrainedModel/1epochModel\")  # Directory that contains the fine tuned model\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "# Assuming `model` and `tokenizer` are your loaded model and tokenizer\n",
        "generator = pipeline(\"text-generation\", model=model, tokenizer=\"LLMTrainedModel/1epochModel/\")\n"
      ],
      "metadata": {
        "id": "P_UxJ_cBmAz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example prompt\n",
        "prompt = f'''###HUMAN: Your task is to process the following fragment of a medical note, retrieving and listing all SNOMED annotations found inside. You will return an \"EMPTY: No SNOMED annotations found in this fragment\" if there are none. For any that you find, you will return a list of: \"text {annotation}\", where 'text' is the part of the text that corresponds to the annotation, and 'annotation' is the corresponding SNOMED-CT annotation. This is the text to analyze:\n",
        "###TEXT: History of Present Illness:\n",
        "Mr. ___ is a ___ man who had severe biliary\n",
        "pancreatitis resulting in pancreatic necrosis for which he was\n",
        "treated with nasojejunal feedings and pancreatic rest.  He had\n",
        "initially had multisystem organ failure, which improved. Mr.\n",
        "___ has a large postnecrotic pseudocyst, which has been\n",
        "drained through a minimally invasive approach into his GI tract.\n",
        " He has some debris, but this is not currently infected. The\n",
        "patient was followed by Dr. ___ in his ___\n",
        "clinic to discuss cholecystectomy. After discussion of all\n",
        "risks, benefits and possible outcomes, patient was scheduled for\n",
        "elective cholecystectomy on ___.'''\n"
      ],
      "metadata": {
        "id": "6vAkasnFmBYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate text\n",
        "generated_texts = generator(prompt, max_length=600, num_return_sequences=1)\n",
        "\n",
        "# Evaluate the generated text against a reference using BLEU, ROUGE, or human evaluation\n",
        "\n",
        "print(generated_texts)\n",
        "\n",
        "# Specify the file path (you can adjust the path as needed)\n",
        "file_path = 'llm_evaluation_outputs.csv'\n",
        "\n",
        "# Open the file in write mode and create a csv.writer object\n",
        "with open(file_path, mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "\n",
        "    # Write a header row, if desired\n",
        "    writer.writerow(['Generated Text'])\n",
        "\n",
        "    # Write each text in the list as a row\n",
        "    for text in generated_texts:\n",
        "        writer.writerow([text])"
      ],
      "metadata": {
        "id": "ZFScVn4enMFK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}